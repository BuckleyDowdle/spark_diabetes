{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier for Diabetes Data\n",
    "\n",
    "**Buckley Dowdle, Latifa Hasan, Luke Moles, Jae Yoon Sung**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('projectModeling') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build evaluators\n",
    "f1_evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='f1')\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27297"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in data\n",
    "data = spark.read.parquet(\"data.parquet\")\n",
    "\n",
    "feats = [\n",
    "       'Gender',\n",
    "       'Age',\n",
    "       'Race_vector',\n",
    "       'Fam_hist',\n",
    "       'Smoke_Cigs',\n",
    "       'BMI',\n",
    "        'HighBP' \n",
    "       ]\n",
    "\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ParticipantID=94054.0, label=1.0, Gender=0.0, Age=50.0, Race=1.0, Fam_hist=0.0, Smoke_Cigs=3.0, BMI=30.4, HighBP=1.0),\n",
       " Row(ParticipantID=94285.0, label=1.0, Gender=0.0, Age=43.0, Race=2.0, Fam_hist=0.0, Smoke_Cigs=1.0, BMI=43.7, HighBP=1.0),\n",
       " Row(ParticipantID=94285.0, label=1.0, Gender=0.0, Age=43.0, Race=2.0, Fam_hist=0.0, Smoke_Cigs=1.0, BMI=43.7, HighBP=1.0),\n",
       " Row(ParticipantID=94285.0, label=1.0, Gender=0.0, Age=43.0, Race=2.0, Fam_hist=0.0, Smoke_Cigs=1.0, BMI=43.7, HighBP=1.0),\n",
       " Row(ParticipantID=94285.0, label=1.0, Gender=0.0, Age=43.0, Race=2.0, Fam_hist=0.0, Smoke_Cigs=1.0, BMI=43.7, HighBP=1.0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check data\n",
    "data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test-train split\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3],seed=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode Race\n",
    "encoder = OneHotEncoder(inputCol=\"Race\", outputCol=\"Race_vector\")\n",
    "\n",
    "#make features into vector\n",
    "assembler = VectorAssembler(inputCols=feats, outputCol='features')\n",
    "\n",
    "#logistic regression model\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "\n",
    "#random forest model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "#model3\n",
    "dt = DecisionTreeClassifier() \n",
    "\n",
    "#create pipelines\n",
    "lr_pipeline = Pipeline(stages=[encoder, assembler, lr])\n",
    "\n",
    "rf_pipeline = Pipeline(stages=[encoder, assembler, rf])\n",
    "\n",
    "dt_pipeline = Pipeline(stages=[encoder, assembler, dt])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameter grid\n",
    "lr_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "lr_crossval = CrossValidator(estimator=lr_pipeline,\n",
    "                          estimatorParamMaps=lr_paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(metricName='f1'),\n",
    "                          numFolds=10,\n",
    "                          seed=314,\n",
    "                          parallelism=7)\n",
    "#fit lr_model\n",
    "lr_model = lr_crossval.fit(trainingData)\n",
    "\n",
    "#make predictions using lr\n",
    "lr_preds = lr_model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic Regression F1 Score: 0.7656557447426172\n",
      "logistic Regression Accuracy: 0.806073070408904\n"
     ]
    }
   ],
   "source": [
    "#test performance\n",
    "lr_f1 = f1_evaluator.evaluate(lr_preds)\n",
    "lr_acc = accuracy_evaluator.evaluate(lr_preds)\n",
    "\n",
    "print('logistic Regression F1 Score: {}'.format(lr_f1))\n",
    "print('logistic Regression Accuracy: {}'.format(lr_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.58      0.26      0.36      1501\n",
      "         2.0       0.83      0.96      0.89      6519\n",
      "         3.0       0.00      0.00      0.00       246\n",
      "\n",
      "    accuracy                           0.81      8266\n",
      "   macro avg       0.47      0.41      0.41      8266\n",
      "weighted avg       0.76      0.81      0.77      8266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = lr_preds.select(['label']).collect()\n",
    "y_pred = lr_preds.select(['prediction']).collect()\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_true, y_pred,zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameter grid\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [30,50]) \\\n",
    "    .addGrid(rf.maxDepth, [30]) \\\n",
    "    .build()\n",
    "\n",
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "rf_crossval = CrossValidator(estimator=rf_pipeline,\n",
    "                          estimatorParamMaps=rf_paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(metricName='f1'),\n",
    "                          numFolds=10,\n",
    "                          seed=314,\n",
    "                          parallelism=23)\n",
    "#fit lr_model\n",
    "rf_model = rf_crossval.fit(trainingData)\n",
    "\n",
    "#make predictions using lr\n",
    "rf_preds = rf_model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest F1 Score: 0.9327214542675151\n",
      "Random Forest Accuracy: 0.9371386394390454\n"
     ]
    }
   ],
   "source": [
    "#test performance\n",
    "rf_f1 = f1_evaluator.evaluate(rf_preds)\n",
    "rf_acc = accuracy_evaluator.evaluate(rf_preds)\n",
    "\n",
    "print('Random Forest F1 Score: {}'.format(rf_f1))\n",
    "print('Random Forest Accuracy: {}'.format(rf_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.922674873244816, 0.9256226772759784]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine scores from crossvalidation\n",
    "rf_model.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimal number of trees\n",
    "## https://stackoverflow.com/questions/36697304/how-to-extract-model-hyper-parameters-from-spark-ml-in-pyspark\n",
    "rf_model.bestModel.stages[-1]._java_obj.parent().getNumTrees()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimal depth\n",
    "rf_model.bestModel.stages[-1]._java_obj.parent().getMaxDepth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.96      0.73      0.83      1465\n",
      "         2.0       0.93      1.00      0.96      6418\n",
      "         3.0       0.99      0.56      0.72       246\n",
      "\n",
      "    accuracy                           0.94      8129\n",
      "   macro avg       0.96      0.76      0.84      8129\n",
      "weighted avg       0.94      0.94      0.93      8129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = rf_preds.select(['label']).collect()\n",
    "y_pred = rf_preds.select(['prediction']).collect()\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RF classifier did reasonably well. In the future we will streamline the data pipeline process, evaluate more metrics, and vary RF parameters. We will also collect data from other years. This should be simple to do, and it will be useful since we had to drop so many duplicates and nulls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameter grid\n",
    "dt_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [2,10,30]) \\\n",
    "    .build()\n",
    "\n",
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "dt_crossval = CrossValidator(estimator=dt_pipeline,\n",
    "                          estimatorParamMaps=dt_paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(metricName='f1'),\n",
    "                          numFolds=10,\n",
    "                          seed=314,\n",
    "                          parallelism=7)\n",
    "#fit dt_model\n",
    "dt_model = dt_crossval.fit(trainingData)\n",
    "\n",
    "#make predictions using dt\n",
    "dt_preds = dt_model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree F1 Score: 0.982259502301589\n",
      "Decision Tree Accuracy: 0.9823372852649407\n"
     ]
    }
   ],
   "source": [
    "#test performance\n",
    "dt_f1 = f1_evaluator.evaluate(dt_preds)\n",
    "dt_acc = accuracy_evaluator.evaluate(dt_preds)\n",
    "\n",
    "print('Decision Tree F1 Score: {}'.format(dt_f1))\n",
    "print('Decision Tree Accuracy: {}'.format(dt_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.689705836666267, 0.8695014276841593, 0.9806770755705196]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine scores from crossvalidation\n",
    "dt_model.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_model.bestModel.stages[-1].getMaxDepth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.96      0.95      0.95      1501\n",
      "         2.0       0.99      0.99      0.99      6519\n",
      "         3.0       1.00      0.93      0.96       246\n",
      "\n",
      "    accuracy                           0.98      8266\n",
      "   macro avg       0.98      0.96      0.97      8266\n",
      "weighted avg       0.98      0.98      0.98      8266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = dt_preds.select(['label']).collect()\n",
    "y_pred = dt_preds.select(['prediction']).collect()\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5559 Spark 3",
   "language": "python",
   "name": "ds5559_spark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
