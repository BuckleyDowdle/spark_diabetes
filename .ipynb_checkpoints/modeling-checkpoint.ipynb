{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier for Diabetes Data\n",
    "\n",
    "**Buckley Dowdle, Latifa Hasan, Luke Moles, Jae Sung**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master('local') \\\n",
    "    .appName('projectModeling') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build evaluators\n",
    "f1_evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='f1')\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27297"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in data\n",
    "data = spark.read.parquet(\"data.parquet\")\n",
    "\n",
    "feats = [\n",
    "       'Gender',\n",
    "       'Age',\n",
    "       'Race_vector',\n",
    "       'Fam_hist',\n",
    "       'Smoke_Cigs',\n",
    "       'BMI',\n",
    "        'HighBP' \n",
    "       ]\n",
    "\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ParticipantID=94054.0, label=1.0, Gender=0.0, Age=50.0, Race=1.0, Fam_hist=0.0, Smoke_Cigs=3.0, BMI=30.4, HighBP=1.0),\n",
       " Row(ParticipantID=94285.0, label=1.0, Gender=0.0, Age=43.0, Race=2.0, Fam_hist=0.0, Smoke_Cigs=1.0, BMI=43.7, HighBP=1.0),\n",
       " Row(ParticipantID=94285.0, label=1.0, Gender=0.0, Age=43.0, Race=2.0, Fam_hist=0.0, Smoke_Cigs=1.0, BMI=43.7, HighBP=1.0),\n",
       " Row(ParticipantID=94285.0, label=1.0, Gender=0.0, Age=43.0, Race=2.0, Fam_hist=0.0, Smoke_Cigs=1.0, BMI=43.7, HighBP=1.0),\n",
       " Row(ParticipantID=94285.0, label=1.0, Gender=0.0, Age=43.0, Race=2.0, Fam_hist=0.0, Smoke_Cigs=1.0, BMI=43.7, HighBP=1.0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check data\n",
    "data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test-train split\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode Race\n",
    "encoder = OneHotEncoder(inputCol=\"Race\", outputCol=\"Race_vector\")\n",
    "\n",
    "#make features into vector\n",
    "assembler = VectorAssembler(inputCols=feats, outputCol='features')\n",
    "\n",
    "#logistic regression model\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "\n",
    "#random forest model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "#model3\n",
    "#model3 = \n",
    "\n",
    "#create pipelines\n",
    "lr_pipeline = Pipeline(stages=[encoder, assembler, lr])\n",
    "\n",
    "rf_pipeline = Pipeline(stages=[encoder, assembler, rf])\n",
    "\n",
    "\n",
    "#pipeline3 = Pipeline(stages=[encoder, assembler, model3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameter grid\n",
    "lr_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "lr_crossval = CrossValidator(estimator=lr_pipeline,\n",
    "                          estimatorParamMaps=lr_paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(metricName='f1'),\n",
    "                          numFolds=10,\n",
    "                          seed=314,\n",
    "                          parallelism=7)\n",
    "#fit lr_model\n",
    "lr_model = lr_crossval.fit(trainingData)\n",
    "\n",
    "#make predictions using lr\n",
    "lr_preds = lr_model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic Regression F1 Score: 0.770537335790141\n",
      "logistic Regression Accuracy: 0.8082307506637703\n"
     ]
    }
   ],
   "source": [
    "#test performance\n",
    "lr_f1 = f1_evaluator.evaluate(lr_preds)\n",
    "lr_acc = accuracy_evaluator.evaluate(lr_preds)\n",
    "\n",
    "print('logistic Regression F1 Score: {}'.format(lr_f1))\n",
    "print('logistic Regression Accuracy: {}'.format(lr_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameter grid\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20, 30]) \\\n",
    "    .addGrid(rf.maxDepth, [2, 5, 8]) \\\n",
    "    .build()\n",
    "\n",
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "rf_crossval = CrossValidator(estimator=rf_pipeline,\n",
    "                          estimatorParamMaps=rf_paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(metricName='f1'),\n",
    "                          numFolds=10,\n",
    "                          seed=314,\n",
    "                          parallelism=7)\n",
    "#fit lr_model\n",
    "rf_model = rf_crossval.fit(trainingData)\n",
    "\n",
    "#make predictions using lr\n",
    "rf_preds = rf_model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test performance\n",
    "rf_f1 = f1_evaluator.evaluate(rf_preds)\n",
    "rf_acc = accuracy_evaluator.evaluate(rf_preds)\n",
    "\n",
    "print('Random Forest F1 Score: {}'.format(rf_f1))\n",
    "print('Random Forest Accuracy: {}'.format(rf_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameter grid\n",
    "model3_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid() \\\n",
    "    .build()\n",
    "\n",
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "model3_crossval = CrossValidator(estimator=model3_pipeline,\n",
    "                          estimatorParamMaps=model3_paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(metricName='f1'),\n",
    "                          numFolds=10,\n",
    "                          seed=314,\n",
    "                          parallelism=7)\n",
    "#fit lr_model\n",
    "model3_model = model3_crossval.fit(trainingData)\n",
    "\n",
    "#make predictions using lr\n",
    "model3_preds = model3_model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7570295194733547"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test performance\n",
    "model3_f1 = f1_evaluator.evaluate(model3_preds)\n",
    "model3_acc = accuracy_evaluator.evaluate(model3_preds)\n",
    "\n",
    "print('logistic Regression F1 Score: {}'.format(model3_f1))\n",
    "print('logistic Regression Accuracy: {}'.format(model3_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RF classifier did reasonably well. In the future we will streamline the data pipeline process, evaluate more metrics, and vary RF parameters. We will also collect data from other years. This should be simple to do, and it will be useful since we had to drop so many duplicates and nulls."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
